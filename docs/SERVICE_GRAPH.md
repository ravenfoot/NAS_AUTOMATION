#!/usr/bin/env bash
# ==============================================================================
# ğŸ–– NAS PROJECT â€” SERVICE GRAPH
# ==============================================================================
# Purpose: Map *when* each automation stage runs and *how* the chain is ordered.
#          This is the temporal blueprint of the NAS control-plane:
#          Boot â†’ Nightly â†’ Weekly â†’ Shutdown.
#
# Audience: SRE / DevOps / backend engineers reviewing deterministic automation,
#           plus hiring managers who want a fast mental model.
#
# Scope: High-level orchestration (systemd) + stage intent + failure posture.
# ==============================================================================

**Core Idea:** This NAS behaves like a small control-plane.  
Each stage is **modular**, **auditable**, and **time-bound** via systemd.

**What this doc answers:**
- What runs **at boot** vs **nightly** vs **weekly** vs **shutdown**.
- Which checks are **sanity-only** vs **maintenance-heavy**.
- Why the order is designed to **minimise blast radius**.

---

## ğŸ”— Related Docs

â”œâ”€â”€ START_HERE.md  
â”œâ”€â”€ NAS_ARCHITECTURE_OVERVIEW.md  
â”œâ”€â”€ SERVICE_GRAPH.md â† You are here  
â”œâ”€â”€ SYSTEM_STATE_BEFORE.md  
â”œâ”€â”€ SYSTEM_STATE_AFTER.md  
â””â”€â”€ PROJECT_JOURNEY.md  

---

## ğŸ§­ Temporal Spine (1-Week Model)

**Boot**  
â†’ Establish trust in the hardware + mounts + baseline security posture.

**Nightly**  
â†’ Low-risk media hygiene.

**Weekly**  
â†’ Security patching, malware scanning, backup integrity, drift auditing.

**Shutdown**  
â†’ Flush + unmount in correct order with log preservation.

---

# 1. âš¡ Boot Sequence

**Design intent:** *Sanity-first, minimal mutation.*  
Boot stages verify that the ship is real before it goes cruising.

**Key assumptions verified:**
- Drives present + healthy enough for service.
- Critical mounts are active.
- UFW is enforcing policy.
- Mullvad interface exists (leak prevention).

**Systemd overview (conceptual):**

local-fs.target
â””â”€ nas-boot-verify.service
â”œâ”€ SMART sweep
â”œâ”€ mount verification
â”œâ”€ BTRFS baseline checks
â””â”€ fstab drift sanity

network.target
â”œâ”€ nas-ufw-check.service
â””â”€ nas-mullvad-check.service


**Stage scripts:**
- `scripts/boot/nas_stage_boot_verify.sh`
- `scripts/ufw/nas_stage_ufw_check.sh`
- `scripts/mullvad/nas_stage_mullvad_check.sh`

---

# 2. ğŸŒ™ Nightly Cycle

**Design intent:** *Low-risk maintenance.*  
Nightly jobs should be safe to rerun and cheap to fail.

**MiniDLNA â€œSurgical Refreshâ€:**
- Stop service  
- Purge `files.db`  
- Preserve `art_cache`  
- Restart  
- Confirm beacons (DLNA + SMB/NFS/Plex)

**Systemd overview:**

nas-dlna-nightly.timer
â””â”€ nas-dlna-nightly.service
â””â”€ nas_stage_dlna_maint.sh


---

# 3. ğŸ” Weekly Maintenance

**Design intent:** *Deep integrity + security posture.*  
This is where you validate the *long-term health narrative* of the NAS.

**Weekly stages:**
- **Security updates** (patch discipline)
- **ClamAV sweep** (threat posture)
- **Borg integrity** (backup trust)
- **Audit** (configuration drift)

**Systemd overview (conceptual):**

nas-update-weekly.timer
â””â”€ nas-update-weekly.service
â””â”€ nas_stage_update.sh

nas-clamav-weekly.timer
â””â”€ nas-clamav-weekly.service
â””â”€ nas_stage_clamav_scan.sh

nas-borg-integrity.timer
â””â”€ nas-borg-integrity.service
â””â”€ nas_stage_borg_integrity.sh

nas-audit-weekly.timer
â””â”€ nas-audit-weekly.service
â””â”€ nas_stage_audit.sh


**Stage scripts:**
- `scripts/update/nas_stage_update.sh`
- `scripts/clamav/nas_stage_clamav_scan.sh`
- `scripts/borg/nas_stage_borg_integrity.sh`
- `scripts/audit/nas_stage_audit.sh`

---

# 4. ğŸ“´ Shutdown Sequence

**Design intent:** *Data-first landing.*  
This stage is about clean dismount of the storage stack.

**Unmount order is intentional:**
1. `/mnt/storage` (overlay)
2. `/mnt/media*` (payload)
3. `/mnt/backup` (last man standing)

**Systemd overview:**

nas-shutdown-stage.service
â””â”€ nas_stage_shutdown.sh
â”œâ”€ sync
â”œâ”€ ordered unmount
â””â”€ final sync


---

# 5. ğŸ“Š Execution Matrix (Fast Scan)

| Stage Script | Boot | Nightly | Weekly | Shutdown |
|-------------|------|---------|--------|----------|
| boot_verify | âœ… |  |  |  |
| ufw_check | âœ… |  |  |  |
| mullvad_check | âœ… |  |  |  |
| dlna_maint |  | âœ… |  |  |
| update |  |  | âœ… |  |
| clamav_scan |  |  | âœ… |  |
| borg_integrity |  |  | âœ… |  |
| audit |  |  | âœ… |  |
| shutdown |  |  |  | âœ… |

---

# 6. ğŸ§¯ Failure Philosophy

**Boot**  
- Logs hard signals.  
- The system may still reach multi-user state, but **trust is downgraded**.

**Nightly**  
- Expected to be safe + repeatable.  
- Failure impact is mostly *client-side convenience*.

**Weekly**  
- Failure indicates *policy drift* or *security/backup risk*.  
- System remains usable, but **admin action is recommended**.

**Shutdown**  
- Prioritises data safety and clean filesystems over speed.

---

# âœ¨ TL;DR

This service graph encodes one idea:

**A home NAS can behave like a tiny SRE-grade control-plane**  
when time, responsibilities, and trust boundaries are explicit.

Boot verifies reality.  
Nightly maintains convenience.  
Weekly defends integrity.  
Shutdown preserves the story.

---

# ==============================================================================
# ğŸ›‘ END
# ==============================================================================
